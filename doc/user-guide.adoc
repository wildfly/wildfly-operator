= WildFly Operator - User Documentation
:toc:               left

This guide documents the various features and capabilites provides by the WildFly Operator.

This guide is complemented by the link:../apis.adoc[API Documentation].

[[basic-install]]
# Basic Install (Phase I)

The features and capabilities of **Basic Install (Phase I)** deals with the provisioning, installation and configuration of a Java application managed by the WildFly Operator.

[[application-image]]
## Specify the Docker Application Image

The `applicationImage` specifies the Docker application image that contains the Java application. The image must have been built
https://github.com/wildfly/wildfly-s2i[WildFly S2I].

[source,yaml]
.Example of application image configuration
----
spec:
  applicationImage: "quay.io/wildfly-quickstarts/wildfly-operator-quickstart:18.0"
----

The `applicationImage` accepts different references to Docker image:

* the name of the image: `quay.io/wildfly-quickstarts/wildfly-operator-quickstart`
* a tag: `quay.io/wildfly-quickstarts/wildfly-operator-quickstart:18.0`
* a digest: `quay.io/wildfly-quickstarts/wildfly-operator-quickstart@sha256:0af38bc38be93116b6a1d86a9c78bd14cd527121970899d719baf78e5dc7bfd2`

[[application-source]]
## Specify the Application Source

The WildFly Operator can also take as input the source code of the application and use https://github.com/openshift/source-to-image[OpenShift Source-to-Image (S2I)] to build the Java applications and create a Docker image from it.

[IMPORTANT]
====
This feature is available on OpenShift only.

It relies on OpenShift-specific resources such as `BuildConfig` and `ImageStreams` that are not available on a plain Kubernetes cluster.
====

[source,yaml]
.Example of application source configuration
----
apiVersion: wildfly.org/v1alpha1
kind: WildFlyServer
metadata:
  name: quickstart
spec:
  applicationSource:
    # The application will be built from the Web application at
    # https://github.com/wildfly/quickstart/tree/19.0.0.Final/helloworld-rs
    sourceRepository:
      url: "https://github.com/wildfly/quickstart.git"
      ref: "19.0.0.Final"
    source2Image:
      builderImage: "wildfly-centos7:19.0"
      runtimeImage: "wildfly-runtime-centos7:19.0"
      env:
      # cloud-server provision a WildFly server that can runs JAX-RS application
      - name: GALLEON_PROVISION_LAYERS
        value: "cloud-server"
      - name: MAVEN_ARGS
        value: "install -pl org.wildfly.quickstarts:helloworld-rs -am "
      - name: ARTIFACT_DIR
        value: "helloworld-rs/target"
  replicas: 1

----

The application will be built from the project at https://github.com/wildfly/quickstart/tree/19.0.0.Final/helloworld-rs (`ref` corresponds to the branch or tag to checkout). The Git repository at https://github.com/wildfly/quickstart contains many Java application but we only want to build and run one of them from the module
`helloworld-rs`. We provide the `MAVEN_ARGS`and `ARTIFACT_DIR` environment variables to S2I to build only this Web application.

Once the application image is build and deployed, the Operator will install it on the OpenShift cluster and create a Route for it.
The application can then be accessed with:

[source,shell]
----
$ curl $(oc get wfly/quickstart -o jsonpath='{.status.hosts[0]}')/helloworld-rs/rest/json
{"result":"Hello World!"}
----

[[source-2-image]]
### Specify Source2Image Configuration

To build the application image, the WildFly S2I images must be installed in the OpenShift cluster before the WildFly Operator deploys the application.
As an example, to install the WildFly 19 S2I images in the `openshift` cluster, you can run the commands:

[source,shell]
----
$ oc import-image wildfly/wildfly-centos7:19.0 --from=quay.io/wildfly/wildfly-centos7:19.0 -n openshift --confirm
$ oc import-image wildfly/wildfly-runtime-centos7:19.0 --from=quay.io/wildfly/wildfly-runtime-centos7:19.0 -n openshift --confirm
----

To be able to build an application from its source, the `builderImage`
must be set in the `source2Image` section. It must correspond to an Image Stream tag of the WildFly S2I Builder image stream.

Optionally, if the `runtimeImage` is set, a 2nd application image will be built that only contains the runtime parts required to run the application on WildFly. This `runtimeImage` must correspond to an Image Stream tag of the WildFly S2I Runtime image stream.

If an image stream tag referenced by a `WildFlyServer` resource is updated (for example, when a new image fixes a CVE), the Operator will detect this image change and trigger the build of a new application image and deploy it on OpenShift.

[[source-webhook]]
### Specify WebHook for Application Code changes

When the Operator deploys an application that is configured with `applicationSource`, it provides Webhooks to trigger new builds of the application image upon code changes.
The `WildFlyServer` custom resource definition supports GitHub and Generic Webhooks. For more information on these Webhooks, please consult the latest https://docs.openshift.com/container-platform/4.3/builds/triggering-builds-build-hooks.html[OpenShift documentation].

The GitHub and Generic Webhooks rely on secret with a key named `WebHookSecretKey` whose value is supplied when invoking the webhook.
When a `WildFlyServer` is created, you can specify the secret that holds this key with either `gitHubWebHookSecret` or `genericHubWebHookSecret`. If these fields are not specified, the Operator will automatically create two secrets named `<wildflyserver name>-github-webhook` and `<wildflyserver name>-generic-webhook` in the same namespace as the `WildFlyServer` resource and use a random string as the value of their `WebHookSecretKey`.

[source,shell]
-----
$ oc describe wfly/quickstart
Name:         quickstart
Namespace:    default
...
Spec:
  Application Source:
    Source Repository:
      Generic Web Hook Secret:  quickstart-generic-webhook
      Git Hub Web Hook Secret:  quickstart-github-webhook
      Ref:                      19.0.0.Final
      URL:                      https://github.com/wildfly/quickstart.git
...

$ oc describe  secret/quickstart-github-webhook
Name:         quickstart-github-webhook
Namespace:    default
...

Type:  Opaque

Data
====
WebHookSecretKey:  36 bytes
-----

To get the value of the `WebHookSecretKey` key, you need to decode it with:

[source,shell]
----
$ oc get secret/quickstart-github-webhook -o jsonpath="{.data.WebHookSecretKey}" | base64 -d
b1ca664f-fe26-4410-87c5-deb35cfb4d64
----

You then need to query the `BuildConfig` created by the Operator that builds the application image from the Git repository to retrieve the corresponding Webhooks:

[source,shell]
----
$ oc describe bc/quickstart
Name:           quickstart
Namespace:      default

...

Webhook GitHub:
        URL:    https://api.crc.testing:6443/apis/build.openshift.io/v1/namespaces/default/buildconfigs/quickstart/webhooks/<secret>/github
Webhook Generic:
        URL:            https://api.crc.testing:6443/apis/build.openshift.io/v1/namespaces/default/buildconfigs/quickstart/webhooks/<secret>/generic
...
----

The actual Webhook URL to use will be formed of the URLs returned by the `BuildConfig` with the `<secret>` part replaced by the value of the corresponding
WebHookSecretKey hook.

In this case, it then would be: https://api.crc.testing:6443/apis/build.openshift.io/v1/namespaces/default/buildconfigs/quickstart/webhooks/b1ca664f-fe26-4410-87c5-deb35cfb4d64/github

You can then configure the GitHub project that hosts the application code with that Webhook.
Any time a change is pushed to the branch that holds the code, this Webhook will be invoked and a new application will be built and deployed on the OpenShift cluster.

[[size]]
## Specify the Size of the Application

The `replicas` specifies the size of the application, i.e. the number of pods that runs the application image.

[source,yaml]
.Example of size configuration
----
spec:
  replicas:2
----

[[storage]]
## Specify the Storage Requirements for the Server Data Directory

The `storage` defines the storage requirements for WildFly's own data.
The application may require persistent storage for some data (e.g. transaction or messaging logs) that must persist across Pod restarts.

If the `storage` spec is empty, an `EmptyDir` volume will be used by each pod of the application (but this volume will not persist after its corresponding pod is stopped).

A `volumeClaimTemplate` cna be specifed to configure `Resources` requirements to store WildFly standalone data directory.
The name of the template is derived from the `WildFlyServer` name. The corresponding volume will be mounted in `ReadWriteOnce` access mode.

The `storage` spec is defined in the link:../apis.adoc#StorageSpec[StorageSpec API Documentation].

[source,yaml]
.Example of storage requirement
----
spec:
  storage:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 3Gi
----

The persistent volume that meets this storage requirement is mounted on the `/wildfly/standalone/data` directory (corresponding to WildFly's `jboss.server.data.dir` path).

[[env]]
## Configure the Application Environment

Environment can be configured using the `env` spec.
Environment variables can come directly from values (such as the `POSTGRESQL_SERVICE_HOST` example below) or from secrets (e.g. the `POSTGRESQL_USER` example below).

[source,yaml]
.Example of environment configuration
----
spec:
  env:
  - name: POSTGRESQL_SERVICE_HOST
    value: postgresql
  - name: POSTGRESQL_SERVICE_PORT
    value: '5432'
  - name: POSTGRESQL_DATABASE
    valueFrom:
      secretKeyRef:
        key: database-name
        name: postgresql
  - name: POSTGRESQL_USER
    valueFrom:
      secretKeyRef:
        key: database-user
        name: postgresql
  - name: POSTGRESQL_PASSWORD
    valueFrom:
      secretKeyRef:
        key: database-password
        name: postgresql
----

[[secret]]
## Configure Secrets

Secrets can be mounted as volumes to be accessed from the application.

The secrets must be created *before* the WildFly Operator deploys the application. For example we can create a secret named `my-secret` with a command such as:

[source,shell]
----
$ kubectl create secret generic my-secret --from-literal=my-key=devuser --from-literal=my-password='my-very-secure-pasword'
----

Once the secret has been created, we can specify its name in the WildFlyServer Spec to have it mounted as a volume in the pods running the application:

[source,yaml]
.Example of mounting secrets
----
spec:
  secrets:
    - my-secret
----

The secrets will then be mounted under `/etc/secrets/<secret name>` and each key/value will be stored in a file (whose name is the key and the content is the value).

[source,shell]
.Secret is mounted as a volume inside the Pod
----
[jboss@quickstart-0 ~]$ ls /etc/secrets/my-secret/
my-key  my-password
[jboss@quickstart-0 ~]$ cat /etc/secrets/my-secret/my-key
devuser
[jboss@quickstart-0 ~]$ cat /etc/secrets/my-secret/my-password
my-very-secure-pasword
----

[[configmaps]]
## Configure ConfigMaps

ConfigMaps can be mounted as volumes to be accessed from the application.

The config maps must be created *before* the WildFly Operator deploys the application. For example we can create a config map named `my-config` with a command such as:

[source,shell]
----
$ kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2
configmap/my-config created
----

Once the config map has been created, we can specify its name in the WildFlyServer Spec to have it mounted as a volume in the pods running the application:

[source,yaml]
.Example of mounting config maps
----
spec:
  configMaps:
  - my-config
----

The config maps will then be mounted under `/etc/configmaps/<config map name>` and each key/value will be stored in a file (whose name is the key and the content is the value).

[source,shell]
.Config Map is mounted as a volume inside the Pod
----
[jboss@quickstart-0 ~]$ ls /etc/configmaps/my-config/
key1 key2
[jboss@quickstart-0 ~]$ cat /etc/configmaps/my-config/key1
value1
[jboss@quickstart-0 ~]$ cat /etc/configmaps/my-config/key2
value2
----

[[standalone-config-map]]
## Bring Your Own Standalone XML Configuation

It is possible to directly provide WildFly standalone configuration instead of the one in the application image (that comes from WildFly S2I).

The standalone XML file must be put in a ConfigMap that is accessible by the operator.
The `standaloneConfigMap` must provide the `name` of this ConfigMap as well as the `key` corresponding to the name of standalone XML file.

[source,yaml]
.Example of bringing its own standalone configuration
----
spec:
  standaloneConfigMap:
    name: clusterbench-config-map
    key: standalone-openshift.xml
----

In this example, the `clusterbench-config-map` must be created *before* the WildFly Operator deploys the application.

[source,shell]
.Example of reating a ConfigMap from a standalone XML file
----
$ kubectl create configmap clusterbench-config-map --from-file examples/clustering/config/standalone-openshift.xml
configmap/clusterbench-config-map created
----

## OpenShift Features

Some Operator features are only available when running on OpenShift if Kubernetes does not provide the required resources to activate these features.

[[http-route-creation]]
### Creation of an HTTP Route

By default, when the Operator runs on OpenShift, it creates an external route to the HTTP port of the Java application.

This route creation can be disabled by setting `disableHTTPRoute` to `true` if you do not wish to create an external route to the Java application.

[source,yaml]
.Example to disable HTTP route
----
spec:
  disableHTTPRoute: true
----

[[scaledown-transaction-recovery]]
## Transaction recovery during scaledown

As the application deployed in the WildFly application server
may use JTA transactions there and the question emerges: what does happen when the cluster is scaled down?
When the number of active WildFly replicas is decreased, still there may be some in-doubt transactions in the transaction log.
When the pod is removed then all the in-progress transactions are stopped and rolled back.
A more troublesome situation occurs when XA transactions are used.
When the XA transaction declares it's prepared it's a promise to finish the transaction successfully.
But the transaction manager which made this promise is running inside the WildFly server.
Then simply shutting down such pod may lead to data inconsistencies or data locks.

It must be ensured that all transactions are finished before the number of replicas is really decreased.
For that purpose, the WildFly Operator provides scale down functionality which verifies if all transactions were finished
and only then marks the pod as clean for termination.

Decreasing the replica size in the `WildFlyServer` customer resource is done at field `WildFlyServer.Spec.Replicas` (see <<size>>).
You can use for example patch command like

```
oc patch wildflyserver <name> -p '[{"op":"replace", "path":"/spec/replicas", "value":0}]' --type json
```

or you can manually edit and change the replica number with `oc edit wildflyserver <name>`.

NOTE: Decreasing replica size at the `StatefulSet` or deleting the Pod itself has no effect and as such changes will be reverted.

WARNING: if you decide to delete whole `WildflyServer` definition (`oc delete wildflyserver <deployment_name>`)
         then no transaction recovery process is started and the pod is terminated regardless of unfinished transactions.
         If you want to remove the deployment in a safe way without data inconsistencies,
         you need first to scale down the number of pods to 0, wait until all pods are terminated
         and only after that you can delete the `WildFlyServer` instance

WARNING: Narayana recovery listener has to be enabled in the WildFly transaction subsystem.
         Otherwise, scaledown transaction recovery processing is skipped for the particular WildFly pod.
         See the link:https://wildscribe.github.io/WildFly/18.0/subsystem/transactions/index.html[`recovery-listener` attribute of the transaction subsystem].

when the scaledown process begins the pod state (`oc get pod <pod_name>``) is still marked as `Running`.
The reason is that that the pod needs to be able to finish all the unfinished transactions and which includes the remote EJB calls that target it.
If you want to observe the state of the scaledown processing you need to observe the status of the `WildFlyServer` instance.
When running `oc describe wildflyserver <name>` you can see the status of the Pods.

The `WildFlyServer.Status.Pods[].State` can be one of the following values:

|===
| Status.Pod.State | Description

| ACTIVE
| The pod is active and processing requests.

| SCALING_DOWN_RECOVERY_INVESTIGATION
| The pod is about to be scaled down. The scale-down process is under investigation about the state of transactions in WildFly.

| SCALING_DOWN_RECOVERY_DIRTY
| The WildFly contains some unfinished transactions. The pod cannot be terminated until they are cleaned.
  The transaction recovery is periodically run at WildFly and it waits the transactions are finished eventually.

| SCALING_DOWN_CLEAN
| The pod was processed by transaction scaled down processing and is marked as clean to be removed from the cluster.

|===


You can observe the overall state of the active and no-active pods by looking at the
`WildFlyServer.Status.'Scalingdown Pods'` and `WildFlyServer.Status.Replicas` fields.
The `'Scalingdown Pods'` defines the number of pods which are about to be terminated when they are clean of unfinished transactions.
The `Replicas` defines the current number of running pods.
The `WildFlyServer.Spec.Replicas` (see <<size>>) defines the desired number of the active pods.
If there are no pods in scaledown process the numbers of `WildFlyServer.Status.Replicas` and `WildFlyServer.Spec.Replicas` are equals.

### Transaction scaledown special cases

#### Heuristics transactions

As it's well-known the transaction may finish either with commit or roll-back.
Unfortunately there is a third outcome which is _unknown_.
Itâ€™s a state when there is no way of automatic transaction recovery and human intervention is needed.
If the transaction is in state of heuristics the pod is marked as `SCALING_DOWN_RECOVERY_DIRTY`
and the administrator needs to manually connect with the `jboss-cli` to the particular WildFly instance
and to resolve the heuristic transaction.

When all the formerly heuristics records are removed from the transaction object store then the operator
marks the pod as `SCALING_DOWN_CLEAN` and the pod is terminated.

#### SCALING_DOWN_CLEAN state and StatefulSet behaviour

There is a special case coming from the design of the `StatefulSet` that ensures that the network hostname is stable
(it does not change on the pod restart). The `StatefulSet` depends on ordering of the pods. The pod are named by the defined order.
The `StatefulSet` then requires the pod-0 not being terminated before the pod-1. First pod-1 is terminated and then pod-0.

From that rule we can observe that if the pod-1 is in state `SCALING_DOWN_RECOVERY_DIRTY` (contains some unfinished, e.g. heuristic transactions)
then if pod-0 is in the state of `SCALING_DOWN_CLEAN` in will be lingering at that state until the pod-1 is terminated.

But even the pod is in state `SCALING_DOWN_CLEAN` the pod is not receiving any new requests
so it's practically idle.

